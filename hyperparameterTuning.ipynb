{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All of the needed packages will be imported here\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#Models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ArXiv-10/arxiv100_parsed.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pre white dwarf eclipsing binary wasp report first bv light curve high resolution spectrum post mass transfer binary star wasp study absolute property extremely low mass white dwarf observed spectrum double lined radial velocity derived effective temperature rotational velocity brighter massive primary found rm eff pm k v sin pm km respectively combined analysis te archive data yielded accurate fundamental parameter program target mass derived accuracy radius better secondary component parameter pm odot r pm r odot rm eff pm k l pm l odot excellent agreement evolutionary sequence helium core white dwarf mass odot indicates star halfway constant luminosity phase result presented article demonstrate wasp el cvn eclipsing binary thin disk formed stable roche lobe overflow channel composed main sequence dwarf spectral type pre white dwarf', 'possible origin khz qpos low mass x ray binary possible origin khz qpos low mass x ray binary proposed recent numerical mhd simulation accretion disk turbulent magnetic field mri definitely show presence two armed spiral structure quasi steady state accretion disk deformed disk two armed c mode n oscillation excited wave wave resonant instability among excited oscillation fundamental radial direction n r higher khz qpo twin qpos first overtone n r radial direction lower khz qpo twin possible cause twin high frequency qpos hfqpos bh x ray binary also discussed', 'effect driving time scale heating coronal arcade context relative importance ac dc heating maintaining temperature corona well constrained aim investigate effect characteristic time scale photospheric driving injection dissipation energy within coronal arcade method conducted three dimensional mhd simulation foot point driving imposed arcade modified typical driving time scale understand efficiency heating obtained using ac dc driver considered implication injected poynting flux nature energy release dissipative regime result driver amplitude complexity long time scale motion able inject much greater poynting flux corona consequently non ideal regime slow stressing motion result greater increase plasma temperature wave like driving dissipative simulation ohmic heating found much significant viscous heating driver parameter space energy dissipation greatest close base arcade magnetic field strength strongest separatrix surface field connectivity change across simulation background field stressed random foot point motion manner typical dc heating study even short time scale driving injected poynting flux large given small amplitude flow considered long time scale driving rate energy injection comparable expected requirement active region heating rate found scale perturbed magnetic field strength total field strength conclusion alongside recent study show power within corona dominated low frequency motion result suggest closed corona dc heating significant ac heating']\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for i in range(0, df['text_filtered'].size):\n",
    "    corpus.append(df['text_filtered'][i])\n",
    "\n",
    "print(corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['label']\n",
    "category = df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_n_model(N, corpus):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(N,2))\n",
    "    return vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(paramX):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(paramX, y, test_size = 0.20)\n",
    "\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    print(X_test.shape, y_test.shape)\n",
    "\n",
    "    print(\"\\nLabel distribution in the training set:\")\n",
    "    print(y_train.value_counts())\n",
    "\n",
    "    print(\"\\nLabel distribution in the test set:\")\n",
    "    print(y_test.value_counts())\n",
    "    return [X_train, X_test, y_train, y_test]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 95632) (80000,)\n",
      "(20000, 95632) (20000,)\n",
      "\n",
      "Label distribution in the training set:\n",
      "hep-ph      8069\n",
      "hep-th      8047\n",
      "physics     8043\n",
      "math        8003\n",
      "cs          8002\n",
      "stat        7997\n",
      "eess        7988\n",
      "cond-mat    7979\n",
      "quant-ph    7944\n",
      "astro-ph    7928\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Label distribution in the test set:\n",
      "astro-ph    2072\n",
      "quant-ph    2056\n",
      "cond-mat    2021\n",
      "eess        2012\n",
      "stat        2003\n",
      "cs          1998\n",
      "math        1997\n",
      "physics     1957\n",
      "hep-th      1953\n",
      "hep-ph      1931\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "[X_train, X_test, y_train, y_test] = split_dataset(tfidf_n_model(1,corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models and parameters\n",
    "model = LogisticRegression()\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "# define grid search\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Place here the results obtained in the previous cell and run\n",
    "modelLR_final = LogisticRegression(penalty='l2', multi_class='auto', max_iter=300, C=1.0, solver='newton-cg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['rbf']}   \n",
    "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n",
    "  \n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train, y_train)\n",
    "# print best parameter after tuning\n",
    "print(grid.best_params_)\n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Place here the results obtained in the previous result and run\n",
    "modelSVC_final = SVC(C=10, kernel='rbf', gamma=0.0001, probability=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train, y_train)\n",
    "# print best parameter after tuning\n",
    "print(grid.best_params_)\n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Place here the results obtained in the previous cell and run\n",
    "modelRFC_final = RandomForestClassifier(max_depth=110, max_features=2, min_samples_leaf=3, min_samples_split=8, n_estimators=200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the final models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLR_final.fit(X_train, y_train)\n",
    "y_pred = modelLR_final.predict(X_test)\n",
    "print(f\"\\n-----{modelLR_final.__class__.__name__}-----\\n\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred, average='macro')}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred, average='macro')}\")\n",
    "print(f\"F1: {f1_score(y_test, y_pred, average='macro')}\")\n",
    "\n",
    "labels = category\n",
    "print(\"--------Classification Report--------\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"--------Confusion Matrix--------\")\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "ax = sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, square=True, fmt=\"g\", cmap=\"viridis\", xticklabels=labels, yticklabels=labels)\n",
    "ax.set_xlabel(\"Predicted\", fontsize=14)\n",
    "ax.set_ylabel(\"True\", fontsize=14)\n",
    "ax.set_title(f\"Confusion Matrix {modelLR_final.__class__.__name__}\", fontsize=16, pad=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSVC_final.fit(X_train, y_train)\n",
    "y_pred = modelSVC_final.predict(X_test)\n",
    "print(f\"\\n-----{modelSVC_final.__class__.__name__}-----\\n\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred, average='macro')}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred, average='macro')}\")\n",
    "print(f\"F1: {f1_score(y_test, y_pred, average='macro')}\")\n",
    "\n",
    "labels = category\n",
    "print(\"--------Classification Report--------\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"--------Confusion Matrix--------\")\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "ax = sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, square=True, fmt=\"g\", cmap=\"viridis\", xticklabels=labels, yticklabels=labels)\n",
    "ax.set_xlabel(\"Predicted\", fontsize=14)\n",
    "ax.set_ylabel(\"True\", fontsize=14)\n",
    "ax.set_title(f\"Confusion Matrix {modelSVC_final.__class__.__name__}\", fontsize=16, pad=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRFC_final.fit(X_train, y_train)\n",
    "y_pred = modelRFC_final.predict(X_test)\n",
    "print(f\"\\n-----{modelRFC_final.__class__.__name__}-----\\n\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred, average='macro')}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred, average='macro')}\")\n",
    "print(f\"F1: {f1_score(y_test, y_pred, average='macro')}\")\n",
    "\n",
    "labels = category\n",
    "print(\"--------Classification Report--------\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"--------Confusion Matrix--------\")\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "ax = sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, square=True, fmt=\"g\", cmap=\"viridis\", xticklabels=labels, yticklabels=labels)\n",
    "ax.set_xlabel(\"Predicted\", fontsize=14)\n",
    "ax.set_ylabel(\"True\", fontsize=14)\n",
    "ax.set_title(f\"Confusion Matrix {modelRFC_final.__class__.__name__}\", fontsize=16, pad=20)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
